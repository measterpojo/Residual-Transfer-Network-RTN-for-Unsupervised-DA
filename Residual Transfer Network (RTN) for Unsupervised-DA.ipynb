{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"A100","machine_shape":"hm","authorship_tag":"ABX9TyOvUrgLu0Vjn3xWchIi24sn"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["**Introduction**"],"metadata":{"id":"msmVKPQZOw1-"}},{"cell_type":"markdown","source":["The Residual Transfer Network (RTN) is a framework designed for domain adaptation, which is the task of adapting a model trained on a source domain to perform well on a target domain with a different data distribution. RTN builds on the concept of residual learning and integrates techniques to reduce the domain gap between the source and target distributions."],"metadata":{"id":"KKmAEcCiOz76"}},{"cell_type":"markdown","source":["  Why is it used for Unsupervised DA ?\n","  "],"metadata":{"id":"nDoFn46rtUc8"}},{"cell_type":"markdown","source":["Residual Transfer Networks (RTNs) shine particularly in unsupervised domain adaptation (UDA) because they are designed to address the lack of labeled data in the target domain by leveraging labeled data from the source domain. Here’s why RTNs are most effective in unsupervised settings, compared to supervised or semi-supervised approaches:"],"metadata":{"id":"ADoKB7ACtYfo"}},{"cell_type":"markdown","source":["**Core Features of RTN**"],"metadata":{"id":"9UgFCyehP4cG"}},{"cell_type":"markdown","source":["1.   Residual Learning:\n","2.   Domain Adaptation Mechanisms:\n","3.   Adversarial Training:\n","4.   Task-Specific Output:"],"metadata":{"id":"2bKsDUV7SSVU"}},{"cell_type":"markdown","source":["**Imports**"],"metadata":{"id":"L9uLhd8iWAgd"}},{"cell_type":"code","source":["import os\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","\n","import torchvision.transforms as transforms\n","from torchvision.datasets import ImageFolder\n","\n","from torch.amp import GradScaler, autocast\n","from torch.utils.data import DataLoader\n","from torchvision import models\n","\n","import kagglehub\n","\n","from itertools import cycle, islice"],"metadata":{"id":"qC3PPWtxWG_Q","executionInfo":{"status":"ok","timestamp":1745967915596,"user_tz":300,"elapsed":3378,"user":{"displayName":"Peter Perez","userId":"12102864046237915346"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","source":["# Setup device-agnostic code\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","device"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"DBS9YWcMWCHQ","executionInfo":{"status":"ok","timestamp":1745967915601,"user_tz":300,"elapsed":3,"user":{"displayName":"Peter Perez","userId":"12102864046237915346"}},"outputId":"cbe36339-ae51-4480-e334-054be3043f1d"},"execution_count":2,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'cuda'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":2}]},{"cell_type":"markdown","source":["**Data Processing**"],"metadata":{"id":"7YcuorlQtdVk"}},{"cell_type":"code","source":["# Download latest version\n","path = kagglehub.dataset_download(\"mei1963/domainnet\")\n","\n","print(\"Path to dataset files:\", path)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"83dKOkJsO0cl","outputId":"f64bdac3-6e43-49f2-821b-d90162503d2f","executionInfo":{"status":"ok","timestamp":1745967919295,"user_tz":300,"elapsed":3692,"user":{"displayName":"Peter Perez","userId":"12102864046237915346"}}},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Path to dataset files: /kaggle/input/domainnet\n"]}]},{"cell_type":"code","execution_count":4,"metadata":{"id":"rAzGn7UAOgWL","executionInfo":{"status":"ok","timestamp":1745967919308,"user_tz":300,"elapsed":14,"user":{"displayName":"Peter Perez","userId":"12102864046237915346"}}},"outputs":[],"source":["\n","def walk_through_dir(dir_path):\n","  \"\"\"\n","  Walks through dir_path returning its contents.\n","  Args:\n","    dir_path (str or pathlib.Path): target directory\n","\n","  Returns:\n","    A print out of:\n","      number of subdiretories in dir_path\n","      number of images (files) in each subdirectory\n","      name of each subdirectory\n","  \"\"\"\n","  for dirpath, dirnames, filenames in os.walk(dir_path):\n","    print(f\"There are {len(dirnames)} directories and {len(filenames)} images in '{dirpath}'.\")"]},{"cell_type":"code","source":["path"],"metadata":{"id":"KszpN5dEstC8","colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"status":"ok","timestamp":1745967919314,"user_tz":300,"elapsed":5,"user":{"displayName":"Peter Perez","userId":"12102864046237915346"}},"outputId":"d9033ba1-1f49-49f1-e75d-59cfb9295397"},"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'/kaggle/input/domainnet'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":5}]},{"cell_type":"code","source":["# walk_through_dir(path)"],"metadata":{"id":"Km9CDjppWcVs","executionInfo":{"status":"ok","timestamp":1745967919327,"user_tz":300,"elapsed":3,"user":{"displayName":"Peter Perez","userId":"12102864046237915346"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["Clipart_path = os.path.join(path, \"DomainNet/clipart\")\n","Real_path = os.path.join(path, \"DomainNet/real\")"],"metadata":{"id":"qSX5TytFslZk","executionInfo":{"status":"ok","timestamp":1745967919331,"user_tz":300,"elapsed":2,"user":{"displayName":"Peter Perez","userId":"12102864046237915346"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["simple_transform = transforms.Compose([\n","    transforms.Resize((224, 224)),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Standard ImageNet normalization\n","])\n"],"metadata":{"id":"ThrLxpdQ5V-K","executionInfo":{"status":"ok","timestamp":1745967919335,"user_tz":300,"elapsed":3,"user":{"displayName":"Peter Perez","userId":"12102864046237915346"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["clipart_dataset = ImageFolder(root=Clipart_path, transform=simple_transform)\n","real_dataset = ImageFolder(root=Real_path, transform=simple_transform)\n","\n","# Example DataLoader with performance optimizations\n","source_data_loader = DataLoader(real_dataset, batch_size=64, shuffle=True,\n","                                num_workers=6, pin_memory=True, drop_last=True)\n","\n","target_dataloader = DataLoader(clipart_dataset, batch_size=64, shuffle=True,\n","                               num_workers=6, pin_memory=True,drop_last=True)"],"metadata":{"id":"xqcrqa53taM1","executionInfo":{"status":"ok","timestamp":1745967920186,"user_tz":300,"elapsed":848,"user":{"displayName":"Peter Perez","userId":"12102864046237915346"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["len(target_dataloader), len(source_data_loader)"],"metadata":{"id":"tqJfRdEVp14F","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1745967920202,"user_tz":300,"elapsed":13,"user":{"displayName":"Peter Perez","userId":"12102864046237915346"}},"outputId":"770950a0-cbb9-4945-89ca-c76d575286f8"},"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(763, 2739)"]},"metadata":{},"execution_count":10}]},{"cell_type":"code","source":["# for batch in target_dataloader:\n","#     print(batch)\n","#     break\n"],"metadata":{"id":"9_Y01zanaND8","executionInfo":{"status":"ok","timestamp":1745967920206,"user_tz":300,"elapsed":3,"user":{"displayName":"Peter Perez","userId":"12102864046237915346"}}},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":["**Models**"],"metadata":{"id":"nIuyFQLvwCgr"}},{"cell_type":"markdown","source":["**Feature extractor**"],"metadata":{"id":"2JbN8he4wDi_"}},{"cell_type":"markdown","source":["ResNet (Residual Network) can be used as the feature extractor in a Residual Transfer Network (RTN). In fact, ResNet is often preferred in domain adaptation tasks because:"],"metadata":{"id":"Sdj_jo1ppbmE"}},{"cell_type":"code","source":["feature_extractor = models.resnet50(pretrained=True)\n","\n","for param in feature_extractor.parameters():\n","    param.requires_grad = False\n","\n","# Unfreeze specific layers\n","for name, param in feature_extractor.named_parameters():\n","    if \"layer4\" in name:  # Example: Unfreeze layer4\n","        param.requires_grad = True\n","\n","\n","feature_extractor.fc  = nn.Identity()"],"metadata":{"id":"QOYmdS_oxNIw","executionInfo":{"status":"ok","timestamp":1745967920612,"user_tz":300,"elapsed":405,"user":{"displayName":"Peter Perez","userId":"12102864046237915346"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"d27d00f1-0429-4da2-830b-f1a0c03d334a"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n"]}]},{"cell_type":"markdown","source":["**Residual Module**"],"metadata":{"id":"04BWBFBVxEoN"}},{"cell_type":"markdown","source":["The Residual Module in a Residual Transfer Network (RTN) is a key component designed to refine and adjust features extracted from the source domain, aligning them with the target domain. This approach leverages residual learning, making the adaptation process more efficient and focused."],"metadata":{"id":"arxqVdSkpWor"}},{"cell_type":"code","source":["class ResidualModule(nn.Module):\n","    def __init__(self, input_dim, output_dim, kernel_size=3, stride=1, padding=1):\n","        super().__init__()\n","        # Linear transformation for residual\n","        self.residual_transform = nn.Linear(input_dim, output_dim)\n","        # Batch Norm\n","        self.batch_norm = nn.LayerNorm(output_dim)\n","        # ReLU activation\n","        self.activation = nn.ReLU()\n","\n","    def forward(self, x):\n","        # Apply residual transformation\n","        residual = self.residual_transform(x)\n","        # print('residual', residual.shape)\n","        batch_norm = self.batch_norm(residual)\n","        activation = self.activation(batch_norm)\n","\n","                # Adjust dimensions if needed\n","        if x.shape[1] != activation.shape[1]:\n","            x = self.residual_transform(x)\n","\n","        adjusted_features = x + activation\n","        return adjusted_features\n","\n","        return adjusted_features\n","\n"],"metadata":{"id":"FiyoxwtYyoZB","executionInfo":{"status":"ok","timestamp":1745967920620,"user_tz":300,"elapsed":6,"user":{"displayName":"Peter Perez","userId":"12102864046237915346"}}},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":["**Domain Discrepancy Minimization**"],"metadata":{"id":"1oEQEGq_hNx4"}},{"cell_type":"markdown","source":["Domain discrepancy minimization is a crucial step in domain adaptation tasks, including in pipelines like the Residual Transfer Network (RTN). It focuses on reducing the difference in data distributions between the source domain (labeled) and the target domain (unlabeled or sparsely labeled). By minimizing this discrepancy, the model learns domain-invariant features that generalize well across both domains."],"metadata":{"id":"Fklq2eNYhSTG"}},{"cell_type":"markdown","source":["The MMD loss is minimized during training to bring the two distributions closer:"],"metadata":{"id":"8LUb5puphbxS"}},{"cell_type":"markdown","source":["MMD\n","2\n","=\n","∥\n","1\n","𝑛\n","𝑠\n","∑\n","𝑖\n","=\n","1\n","𝑛\n","𝑠\n","𝜙\n","(\n","𝑥\n","𝑖\n","𝑠\n",")\n","−\n","1\n","𝑛\n","𝑡\n","∑\n","𝑖\n","=\n","1\n","𝑛\n","𝑡\n","𝜙\n","(\n","𝑥\n","𝑖\n","𝑡\n",")\n","∥\n","2"],"metadata":{"id":"9n2qCAx7hXuY"}},{"cell_type":"code","source":["class MMDLoss(nn.Module):\n","  def __init__(self, kernel='rbf', gamma=1.0):\n","    super().__init__()\n","    self.kernel = kernel\n","    self.gamma = gamma\n","\n","  def pairwise_distance(self, x, y):\n","    x_norm = torch.sum(x**2, dim=-1)\n","    y_norm = torch.sum(y**2, dim=-1)\n","    dist = x_norm + y_norm - 2 * torch.matmul(x, y.t())\n","\n","    return torch.clamp(dist, min=0.0)\n","\n","  def rbf_kernel(self, x, y):\n","    dist = self.pairwise_distance(x, y)\n","    return torch.exp(-self.gamma * dist)\n","\n","  def linear_kernel(self, x, y):\n","    return torch.matmul(x, y.t())\n","\n","  def forward(self, source_features, target_features):\n","      \"\"\"\n","      Compute the Maximum Mean Discrepancy (MMD) loss.\n","\n","      Args:\n","          source_features (torch.Tensor): Features from source domain.\n","          target_features (torch.Tensor): Features from target domain.\n","\n","      Returns:\n","          torch.Tensor: MMD loss.\n","      \"\"\"\n","      if self.kernel == 'rbf':\n","          kernel_fn = self.rbf_kernel\n","      elif self.kernel == 'linear':\n","          kernel_fn = self.linear_kernel\n","      else:\n","          raise ValueError(\"Unsupported kernel type. Choose 'linear' or 'rbf'.\")\n","\n","      xx = kernel_fn(source_features, source_features).mean()\n","      yy = kernel_fn(target_features, target_features).mean()\n","      xy = kernel_fn(source_features, target_features).mean()\n","      mmd_loss = xx + yy - 2.0 * xy\n","      return torch.clamp(mmd_loss, min=0.0)"],"metadata":{"id":"0ozH06NFhsYJ","executionInfo":{"status":"ok","timestamp":1745967920628,"user_tz":300,"elapsed":6,"user":{"displayName":"Peter Perez","userId":"12102864046237915346"}}},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":["**Task-Specific Classifier**"],"metadata":{"id":"7uE3wyd4qtnu"}},{"cell_type":"code","source":["class TaskClassifier(nn.Module):\n","  def __init__(self, input_dim, num_classes, dropout=0.5,*args, **kwargs) -> None:\n","     super().__init__(*args, **kwargs)\n","     self.fc1 = nn.Linear(input_dim, 256)\n","     self.relu = nn.ReLU()\n","     self.dropout = nn.Dropout(dropout)\n","     self.fc2 = nn.Linear(256, num_classes)\n","\n","  def forward(self, x):\n","    x = self.fc1(x)\n","    # print('after fc1', x.shape)\n","    x = self.relu(x)\n","    x = self.dropout(x)\n","    x = self.fc2(x)\n","    return x"],"metadata":{"id":"kkTP4z67q1He","executionInfo":{"status":"ok","timestamp":1745967920634,"user_tz":300,"elapsed":4,"user":{"displayName":"Peter Perez","userId":"12102864046237915346"}}},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":["By encapsulating its components into a single class, you ensure modularity and easy integration into your training loop."],"metadata":{"id":"Taq2iGgPwN9Z"}},{"cell_type":"code","source":["# Components: Feature Extractor, Residual Module, Classifier\n","class RTN(nn.Module):\n","  def __init__(self, feature_extractor, residual_module, classifier):\n","    super().__init__()\n","    self.feature_extractor = feature_extractor\n","    self.residual_module = residual_module\n","    self.classifier = classifier\n","\n","  def forward(self, x):\n","      features = self.feature_extractor(x)\n","      # print('feature shape',features.shape)\n","      aligned_features = self.residual_module(features)\n","      predictions = self.classifier(aligned_features)\n","      return aligned_features, predictions"],"metadata":{"id":"bWQsLbRsvZJK","executionInfo":{"status":"ok","timestamp":1745967920640,"user_tz":300,"elapsed":5,"user":{"displayName":"Peter Perez","userId":"12102864046237915346"}}},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":["**Training Loop**"],"metadata":{"id":"bKaTjvYUpz5b"}},{"cell_type":"code","source":["feature_extractor = feature_extractor.to(device)\n","residual_module = ResidualModule(2048, 512).to(device)  # Residual module\n","classifier = TaskClassifier(input_dim=512, num_classes=345).to(device)\n","\n","mmd_loss_fn = MMDLoss(kernel='rbf', gamma=1.0).to(device)\n","classification_loss_fn = nn.CrossEntropyLoss()\n","\n","optimizer = optim.Adam(list(feature_extractor.parameters()) + list(classifier.parameters()), lr=0.001)\n","\n","\n","model = RTN(feature_extractor, residual_module, classifier)"],"metadata":{"id":"H6eOasDxrJL9","executionInfo":{"status":"ok","timestamp":1745967920802,"user_tz":300,"elapsed":29,"user":{"displayName":"Peter Perez","userId":"12102864046237915346"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["scaler = GradScaler()\n","\n","torch.cuda.empty_cache()\n","# torch.cuda.reset_peak_memory_stats()\n","\n","target_dataloader_cycle = cycle(target_dataloader)\n","\n","epochs = 5\n","for epoch in range(epochs):\n","  model.train()\n","  total_classification_loss = 0.0\n","  total_mmd_loss = 0.0\n","\n","  for source_data, target_data in zip(source_data_loader, islice(target_dataloader_cycle, len(source_data_loader))):\n","\n","      # Supervised source domain data\n","      source_inputs, source_labels = source_data\n","      source_inputs, source_labels = source_inputs.to(device), source_labels.to(device)\n","\n","      # Unsupervised target domain data\n","      target_inputs, _ = target_data  # Target data has no labels\n","      target_inputs = target_inputs.to(device)\n","\n","      optimizer.zero_grad()\n","\n","      # Enable mixed precision\n","      with autocast(device_type='cuda'):\n","          source_features, source_predictions = model(source_inputs)\n","          target_features, _ = model(target_inputs)\n","\n","          classification_loss = classification_loss_fn(source_predictions, source_labels)\n","          mmd_loss = mmd_loss_fn(source_features, target_features)\n","\n","          total_loss = classification_loss + mmd_loss\n","\n","      # Backward pass with scaled gradients\n","      scaler.scale(total_loss).backward()\n","      scaler.step(optimizer)\n","      scaler.update()\n","\n","      total_classification_loss += classification_loss.item()\n","      total_mmd_loss += mmd_loss.item()\n","  print(f\"Epoch [{epoch+1}/{epochs}], Classification Loss: {total_classification_loss:.4f}, MMD Loss: {total_mmd_loss:.4f}\")"],"metadata":{"id":"3GmoPRB-u-m1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1745968885971,"user_tz":300,"elapsed":965159,"user":{"displayName":"Peter Perez","userId":"12102864046237915346"}},"outputId":"98e7fb51-9b8d-46a8-e611-78ff49a85c8f"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch [1/5], Classification Loss: 6308.9418, MMD Loss: 15.3575\n","Epoch [2/5], Classification Loss: 3660.6271, MMD Loss: 2.8869\n","Epoch [3/5], Classification Loss: 2920.0178, MMD Loss: 1.5874\n","Epoch [4/5], Classification Loss: 2421.1814, MMD Loss: 1.2705\n","Epoch [5/5], Classification Loss: 2039.2852, MMD Loss: 1.0331\n"]}]},{"cell_type":"markdown","source":["The classification loss is steadily decreasing, which suggests that your model is learning well. Meanwhile, the MMD loss is gradually reducing, meaning the feature distributions between your source and target domains are aligning more closely."],"metadata":{"id":"_VN7tJAK3h1V"}}]}